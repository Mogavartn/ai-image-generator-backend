#!/usr/bin/env python3
"""
G√©n√©rateur d'Images IA - Backend Python
Support pour mod√®les Flux et Hunyuan avec LoRAs
"""

import os
import json
import time
import hashlib
import requests
import tempfile
import shutil
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional, Tuple
from urllib.parse import urlparse

import torch
from flask import Flask, request, jsonify, send_file
from flask_cors import CORS
from werkzeug.utils import secure_filename
import safetensors.torch as st

# Imports sp√©cifiques aux mod√®les (√† adapter selon vos librairies)
try:
    from diffusers import FluxPipeline, DiffusionPipeline
    DIFFUSERS_AVAILABLE = True
except ImportError:
    DIFFUSERS_AVAILABLE = False
    print("‚ö†Ô∏è  Diffusers non install√©. Installez avec: pip install diffusers")

try:
    # Import hypoth√©tique pour Hunyuan (remplacez par la vraie librairie)
    from hunyuan_dit import HunyuanPipeline
    HUNYUAN_AVAILABLE = True
except ImportError:
    HUNYUAN_AVAILABLE = False
    print("‚ö†Ô∏è  Librairie Hunyuan non trouv√©e. Veuillez l'installer.")

app = Flask(__name__)
CORS(app, origins=['https://webinterface-imageai.onrender.com'])

# Configuration
app.config.update(
    MAX_CONTENT_LENGTH=2 * 1024 * 1024 * 1024,  # 2GB max
    UPLOAD_FOLDER=tempfile.mkdtemp(prefix="ai_gen_"),
    RESULTS_FOLDER="./generated_images",
    ALLOWED_EXTENSIONS={'.safetensors'},
    MAX_IMAGES=4,
    DEFAULT_STEPS=30,
    DEFAULT_GUIDANCE=7.5
)

# Variables globales pour les pipelines charg√©s
loaded_pipelines = {}
temp_files = set()

class ImageGenerator:
    """Gestionnaire principal pour la g√©n√©ration d'images"""
    
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.temp_dir = Path(app.config['UPLOAD_FOLDER'])
        self.results_dir = Path(app.config['RESULTS_FOLDER'])
        self.results_dir.mkdir(exist_ok=True)
        
        print(f"üñ•Ô∏è  Device: {self.device}")
        print(f"üìÅ Dossier temporaire: {self.temp_dir}")
        print(f"üìÅ Dossier r√©sultats: {self.results_dir}")
    
    def validate_safetensors(self, file_path: Path) -> bool:
        """Valide qu'un fichier safetensors est lisible"""
        try:
            st.load_file(str(file_path))
            return True
        except Exception as e:
            print(f"‚ùå Erreur validation safetensors: {e}")
            return False
    
    def download_lora(self, url: str) -> Optional[Path]:
        """T√©l√©charge un LoRA depuis une URL"""
        try:
            print(f"üì• T√©l√©chargement LoRA: {url}")
            
            # G√©n√©rer un nom de fichier s√©curis√©
            parsed = urlparse(url)
            filename = os.path.basename(parsed.path) or "lora.safetensors"
            if not filename.endswith('.safetensors'):
                filename += '.safetensors'
            
            local_path = self.temp_dir / f"lora_{hashlib.md5(url.encode()).hexdigest()[:8]}_{filename}"
            
            # T√©l√©charger avec timeout
            response = requests.get(url, timeout=300, stream=True)
            response.raise_for_status()
            
            with open(local_path, 'wb') as f:
                shutil.copyfileobj(response.raw, f)
            
            # Valider le fichier t√©l√©charg√©
            if self.validate_safetensors(local_path):
                temp_files.add(str(local_path))
                print(f"‚úÖ LoRA t√©l√©charg√©: {local_path}")
                return local_path
            else:
                local_path.unlink(missing_ok=True)
                print(f"‚ùå LoRA invalide: {url}")
                return None
                
        except Exception as e:
            print(f"‚ùå Erreur t√©l√©chargement LoRA {url}: {e}")
            return None
    
    def load_flux_pipeline(self, model_path: Path, lora_paths: List[Path]) -> Optional[object]:
        """Charge un pipeline Flux avec LoRAs"""
        if not DIFFUSERS_AVAILABLE:
            raise ValueError("Diffusers non disponible")
        
        try:
            print(f"üîÑ Chargement mod√®le Flux: {model_path}")
            
            # Charger le pipeline de base
            pipeline = FluxPipeline.from_pretrained(
                str(model_path),
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                device_map="auto" if self.device == "cuda" else None
            )
            
            # Charger les LoRAs
            for lora_path in lora_paths:
                print(f"üîó Ajout LoRA: {lora_path}")
                pipeline.load_lora_weights(str(lora_path))
            
            pipeline = pipeline.to(self.device)
            return pipeline
            
        except Exception as e:
            print(f"‚ùå Erreur chargement Flux: {e}")
            return None
    
    def load_hunyuan_pipeline(self, model_path: Path, lora_paths: List[Path]) -> Optional[object]:
        """Charge un pipeline Hunyuan avec LoRAs"""
        if not HUNYUAN_AVAILABLE:
            raise ValueError("Librairie Hunyuan non disponible")
        
        try:
            print(f"üîÑ Chargement mod√®le Hunyuan: {model_path}")
            
            # Adaptation n√©cessaire selon l'API Hunyuan r√©elle
            pipeline = HunyuanPipeline.from_pretrained(
                str(model_path),
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32
            )
            
            # Charger les LoRAs (API √† adapter)
            for lora_path in lora_paths:
                print(f"üîó Ajout LoRA Hunyuan: {lora_path}")
                # pipeline.load_lora_weights(str(lora_path))  # √Ä adapter
            
            pipeline = pipeline.to(self.device)
            return pipeline
            
        except Exception as e:
            print(f"‚ùå Erreur chargement Hunyuan: {e}")
            return None
    
    def parse_dimensions(self, dimensions_str: str) -> Tuple[int, int]:
        """Parse les dimensions au format 'WxH'"""
        try:
            width, height = map(int, dimensions_str.split('x'))
            return width, height
        except:
            return 512, 512
    
    def generate_images(self, 
                       model_path: Path,
                       model_type: str,
                       prompt: str,
                       lora_paths: List[Path] = None,
                       num_images: int = 1,
                       dimensions: str = "512x512",
                       steps: int = 30,
                       guidance_scale: float = 7.5,
                       output_dir: str = None) -> List[Dict]:
        """G√©n√®re les images selon les param√®tres"""
        
        if lora_paths is None:
            lora_paths = []
        
        if output_dir is None:
            output_dir = self.results_dir
        else:
            output_dir = Path(output_dir)
            output_dir.mkdir(parents=True, exist_ok=True)
        
        # Cr√©er une cl√© unique pour le pipeline
        pipeline_key = f"{model_type}_{model_path.stem}_{len(lora_paths)}"
        
        # Charger ou r√©cup√©rer le pipeline
        if pipeline_key not in loaded_pipelines:
            if model_type.lower() == "flux":
                pipeline = self.load_flux_pipeline(model_path, lora_paths)
            elif model_type.lower() == "hunyuan":
                pipeline = self.load_hunyuan_pipeline(model_path, lora_paths)
            else:
                raise ValueError(f"Type de mod√®le non support√©: {model_type}")
            
            if pipeline is None:
                raise ValueError(f"Impossible de charger le pipeline {model_type}")
            
            loaded_pipelines[pipeline_key] = pipeline
        else:
            pipeline = loaded_pipelines[pipeline_key]
            print(f"‚ôªÔ∏è  R√©utilisation pipeline: {pipeline_key}")
        
        # Parser les dimensions
        width, height = self.parse_dimensions(dimensions)
        
        # G√©n√©rer les images
        results = []
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        for i in range(num_images):
            try:
                print(f"üé® G√©n√©ration image {i+1}/{num_images}")
                start_time = time.time()
                
                # G√©n√©rer l'image
                with torch.no_grad():
                    image = pipeline(
                        prompt=prompt,
                        width=width,
                        height=height,
                        num_inference_steps=steps,
                        guidance_scale=guidance_scale,
                        generator=torch.Generator(device=self.device).manual_seed(
                            int(time.time()) + i
                        )
                    ).images[0]
                
                generation_time = time.time() - start_time
                
                # Sauvegarder l'image
                filename = f"generated_{timestamp}_{i+1:03d}.png"
                image_path = output_dir / filename
                image.save(str(image_path))
                
                results.append({
                    "filename": filename,
                    "path": str(image_path),
                    "dimensions": f"{width}x{height}",
                    "generation_time": round(generation_time, 2),
                    "url": f"/api/image/{filename}"
                })
                
                print(f"‚úÖ Image {i+1} g√©n√©r√©e en {generation_time:.2f}s: {filename}")
                
            except Exception as e:
                print(f"‚ùå Erreur g√©n√©ration image {i+1}: {e}")
                results.append({
                    "error": str(e),
                    "index": i+1
                })
        
        return results

# Instance globale du g√©n√©rateur
generator = ImageGenerator()

@app.route('/api/generate', methods=['POST'])
def generate_images():
    """Endpoint principal pour g√©n√©rer des images"""
    try:
        print("üì® Nouvelle requ√™te de g√©n√©ration")
        
        # Validation des donn√©es re√ßues
        if 'model_file' not in request.files:
            return jsonify({"success": False, "error": "Aucun fichier mod√®le fourni"}), 400
        
        model_file = request.files['model_file']
        if model_file.filename == '':
            return jsonify({"success": False, "error": "Nom de fichier invalide"}), 400
        
        # Sauvegarder le fichier mod√®le
        filename = secure_filename(model_file.filename)
        if not filename.endswith('.safetensors'):
            return jsonify({"success": False, "error": "Format de fichier non support√©"}), 400
        
        model_path = generator.temp_dir / f"model_{int(time.time())}_{filename}"
        model_file.save(str(model_path))
        temp_files.add(str(model_path))
        
        # Valider le mod√®le
        if not generator.validate_safetensors(model_path):
            return jsonify({"success": False, "error": "Fichier safetensors invalide"}), 400
        
        # R√©cup√©rer les param√®tres
        model_type = request.form.get('model_type', 'flux')
        prompt = request.form.get('prompt', '').strip()
        lora_urls = request.form.get('lora_urls', '').strip()
        output_dir = request.form.get('output_dir', str(generator.results_dir))
        image_count = min(int(request.form.get('image_count', 1)), app.config['MAX_IMAGES'])
        dimensions = request.form.get('dimensions', '512x512')
        steps = int(request.form.get('steps', app.config['DEFAULT_STEPS']))
        guidance = float(request.form.get('guidance', app.config['DEFAULT_GUIDANCE']))
        
        if not prompt:
            return jsonify({"success": False, "error": "Prompt requis"}), 400
        
        print(f"üéØ Param√®tres: {model_type}, {image_count} images, {dimensions}")
        
        # T√©l√©charger les LoRAs
        lora_paths = []
        if lora_urls:
            urls = [url.strip() for url in lora_urls.split('\n') if url.strip()]
            for url in urls:
                lora_path = generator.download_lora(url)
                if lora_path:
                    lora_paths.append(lora_path)
        
        # G√©n√©rer les images
        results = generator.generate_images(
            model_path=model_path,
            model_type=model_type,
            prompt=prompt,
            lora_paths=lora_paths,
            num_images=image_count,
            dimensions=dimensions,
            steps=steps,
            guidance_scale=guidance,
            output_dir=output_dir
        )
        
        # Filtrer les r√©sultats avec erreurs
        successful_results = [r for r in results if 'error' not in r]
        failed_results = [r for r in results if 'error' in r]
        
        response = {
            "success": True,
            "images": successful_results,
            "total_generated": len(successful_results),
            "total_requested": image_count
        }
        
        if failed_results:
            response["errors"] = failed_results
        
        print(f"‚úÖ G√©n√©ration termin√©e: {len(successful_results)}/{image_count} images")
        return jsonify(response)
        
    except Exception as e:
        print(f"‚ùå Erreur serveur: {e}")
        return jsonify({"success": False, "error": str(e)}), 500

@app.route('/api/image/<filename>')
def serve_image(filename):
    """Servir les images g√©n√©r√©es"""
    try:
        image_path = generator.results_dir / secure_filename(filename)
        if image_path.exists():
            return send_file(str(image_path))
        else:
            return jsonify({"error": "Image non trouv√©e"}), 404
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/cleanup', methods=['POST'])
def cleanup_temp_files():
    """Nettoyer les fichiers temporaires"""
    try:
        cleaned_count = 0
        
        # Nettoyer les fichiers temporaires track√©s
        for file_path in list(temp_files):
            try:
                if os.path.exists(file_path):
                    os.unlink(file_path)
                    cleaned_count += 1
                temp_files.discard(file_path)
            except Exception as e:
                print(f"‚ö†Ô∏è  Erreur suppression {file_path}: {e}")
        
        # Nettoyer le dossier temporaire
        try:
            for item in generator.temp_dir.iterdir():
                if item.is_file():
                    item.unlink()
                    cleaned_count += 1
                elif item.is_dir():
                    shutil.rmtree(item)
                    cleaned_count += 1
        except Exception as e:
            print(f"‚ö†Ô∏è  Erreur nettoyage dossier temp: {e}")
        
        # Vider le cache des pipelines si demand√©
        global loaded_pipelines
        if request.json and request.json.get('clear_models', False):
            loaded_pipelines.clear()
            torch.cuda.empty_cache() if torch.cuda.is_available() else None
            print("üßπ Cache mod√®les vid√©")
        
        print(f"üßπ Nettoyage termin√©: {cleaned_count} √©l√©ments supprim√©s")
        return jsonify({
            "success": True,
            "cleaned_files": cleaned_count,
            "message": f"{cleaned_count} fichiers temporaires supprim√©s"
        })
        
    except Exception as e:
        print(f"‚ùå Erreur nettoyage: {e}")
        return jsonify({"success": False, "error": str(e)}), 500

@app.route('/api/status')
def get_status():
    """Status de l'application"""
    return jsonify({
        "status": "online",
        "device": generator.device,
        "loaded_models": len(loaded_pipelines),
        "temp_files": len(temp_files),
        "diffusers_available": DIFFUSERS_AVAILABLE,
        "hunyuan_available": HUNYUAN_AVAILABLE,
        "cuda_available": torch.cuda.is_available(),
        "memory_used": torch.cuda.memory_allocated() if torch.cuda.is_available() else 0
    })

@app.route('/')
def index():
    """Page d'accueil - servir l'interface web"""
    return '''
    <!DOCTYPE html>
    <html>
    <head>
        <title>G√©n√©rateur d'Images IA</title>
    </head>
    <body>
        <h1>üé® G√©n√©rateur d'Images IA - Backend</h1>
        <p>Le backend Python est en fonctionnement!</p>
        <ul>
            <li><a href="/api/status">Status de l'API</a></li>
            <li><strong>POST /api/generate</strong> - G√©n√©rer des images</li>
            <li><strong>POST /api/cleanup</strong> - Nettoyer les fichiers temporaires</li>
            <li><strong>GET /api/image/&lt;filename&gt;</strong> - Servir les images</li>
        </ul>
        <p>Utilisez l'interface web HTML pour interagir avec cette API.</p>
    </body>
    </html>
    '''

if __name__ == '__main__':
    print("üöÄ D√©marrage du serveur de g√©n√©ration d'images IA")
    print(f"üìä Device: {generator.device}")
    print(f"üîß Diffusers: {'‚úÖ' if DIFFUSERS_AVAILABLE else '‚ùå'}")
    print(f"üîß Hunyuan: {'‚úÖ' if HUNYUAN_AVAILABLE else '‚ùå'}")
    
    # Configuration pour le d√©veloppement
    debug_mode = os.getenv('FLASK_DEBUG', 'False').lower() == 'true'
    port = int(os.getenv('PORT', 5000))
    host = os.getenv('HOST', '0.0.0.0' if not debug_mode else '127.0.0.1')
    
    app.run(
        host=host,
        port=port,
        debug=debug_mode,
        threaded=True
    )